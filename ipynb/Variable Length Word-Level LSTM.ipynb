{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 326073\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np\n",
    "\n",
    "text = open('input.txt').read() # doom lyrics\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data):\n",
    "    corpus = data.lower().split(\"\\n\")    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1 \n",
    "    \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        \n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,   \n",
    "                          maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    \n",
    "    return predictors, label, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 50, input_length=input_len))\n",
    "    model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(75)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(total_words, activation='softmax'))    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    model.fit(predictors, label, epochs=100, verbose=1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1713/1713 [==============================] - 213s 125ms/step - loss: 7.2658\n",
      "Epoch 2/100\n",
      "1713/1713 [==============================] - 192s 112ms/step - loss: 6.8363\n",
      "Epoch 3/100\n",
      "1713/1713 [==============================] - 190s 111ms/step - loss: 6.5843\n",
      "Epoch 4/100\n",
      "1713/1713 [==============================] - 175s 102ms/step - loss: 6.3574\n",
      "Epoch 5/100\n",
      "1713/1713 [==============================] - 165s 96ms/step - loss: 6.1585\n",
      "Epoch 6/100\n",
      "1713/1713 [==============================] - 164s 96ms/step - loss: 5.9762\n",
      "Epoch 7/100\n",
      "1713/1713 [==============================] - 199s 116ms/step - loss: 5.8041\n",
      "Epoch 8/100\n",
      "1713/1713 [==============================] - 205s 120ms/step - loss: 5.6358\n",
      "Epoch 9/100\n",
      "1713/1713 [==============================] - 218s 127ms/step - loss: 5.4720\n",
      "Epoch 10/100\n",
      "1713/1713 [==============================] - 235s 137ms/step - loss: 5.3096\n",
      "Epoch 11/100\n",
      "1713/1713 [==============================] - 191s 111ms/step - loss: 5.1476\n",
      "Epoch 12/100\n",
      "1713/1713 [==============================] - 205s 120ms/step - loss: 4.9906\n",
      "Epoch 13/100\n",
      "1713/1713 [==============================] - 217s 126ms/step - loss: 4.8411\n",
      "Epoch 14/100\n",
      "1713/1713 [==============================] - 192s 112ms/step - loss: 4.6933\n",
      "Epoch 15/100\n",
      "1713/1713 [==============================] - 212s 124ms/step - loss: 4.5608\n",
      "Epoch 16/100\n",
      "1713/1713 [==============================] - 225s 131ms/step - loss: 4.4394\n",
      "Epoch 17/100\n",
      "1713/1713 [==============================] - 234s 137ms/step - loss: 4.3178\n",
      "Epoch 18/100\n",
      "1713/1713 [==============================] - 267s 156ms/step - loss: 4.2041\n",
      "Epoch 19/100\n",
      "1713/1713 [==============================] - 212s 124ms/step - loss: 4.0953\n",
      "Epoch 20/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 3.9880\n",
      "Epoch 21/100\n",
      "1713/1713 [==============================] - 164s 96ms/step - loss: 3.8896\n",
      "Epoch 22/100\n",
      "1713/1713 [==============================] - 170s 99ms/step - loss: 3.7991\n",
      "Epoch 23/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 3.7045\n",
      "Epoch 24/100\n",
      "1713/1713 [==============================] - 163s 95ms/step - loss: 3.6222\n",
      "Epoch 25/100\n",
      "1713/1713 [==============================] - 163s 95ms/step - loss: 3.5365\n",
      "Epoch 26/100\n",
      "1713/1713 [==============================] - 163s 95ms/step - loss: 3.4566\n",
      "Epoch 27/100\n",
      "1713/1713 [==============================] - 163s 95ms/step - loss: 3.3790\n",
      "Epoch 28/100\n",
      "1713/1713 [==============================] - 163s 95ms/step - loss: 3.3077\n",
      "Epoch 29/100\n",
      "1713/1713 [==============================] - 165s 96ms/step - loss: 3.2355\n",
      "Epoch 30/100\n",
      "1713/1713 [==============================] - 165s 96ms/step - loss: 3.1616\n",
      "Epoch 31/100\n",
      "1713/1713 [==============================] - 165s 96ms/step - loss: 3.0885\n",
      "Epoch 32/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 3.0295\n",
      "Epoch 33/100\n",
      "1713/1713 [==============================] - 164s 96ms/step - loss: 2.9621\n",
      "Epoch 34/100\n",
      "1713/1713 [==============================] - 164s 96ms/step - loss: 2.8952\n",
      "Epoch 35/100\n",
      "1713/1713 [==============================] - 165s 96ms/step - loss: 2.8408\n",
      "Epoch 36/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 2.7898\n",
      "Epoch 37/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 2.7297\n",
      "Epoch 38/100\n",
      "1713/1713 [==============================] - 165s 96ms/step - loss: 2.6790\n",
      "Epoch 39/100\n",
      "1713/1713 [==============================] - 185s 108ms/step - loss: 2.6270\n",
      "Epoch 40/100\n",
      "1713/1713 [==============================] - 182s 106ms/step - loss: 2.5737\n",
      "Epoch 41/100\n",
      "1713/1713 [==============================] - 240s 140ms/step - loss: 2.5305\n",
      "Epoch 42/100\n",
      "1713/1713 [==============================] - 216s 126ms/step - loss: 2.4890\n",
      "Epoch 43/100\n",
      "1713/1713 [==============================] - 189s 110ms/step - loss: 2.4414\n",
      "Epoch 44/100\n",
      "1713/1713 [==============================] - 189s 110ms/step - loss: 2.4022\n",
      "Epoch 45/100\n",
      "1713/1713 [==============================] - 192s 112ms/step - loss: 2.3486\n",
      "Epoch 46/100\n",
      "1713/1713 [==============================] - ETA: 0s - loss: 2.323 - 215s 125ms/step - loss: 2.3236\n",
      "Epoch 47/100\n",
      "1713/1713 [==============================] - 175s 102ms/step - loss: 2.2815\n",
      "Epoch 48/100\n",
      "1713/1713 [==============================] - 174s 102ms/step - loss: 2.2495\n",
      "Epoch 49/100\n",
      "1713/1713 [==============================] - 172s 101ms/step - loss: 2.2116\n",
      "Epoch 50/100\n",
      "1713/1713 [==============================] - 171s 100ms/step - loss: 2.1684\n",
      "Epoch 51/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 2.1387\n",
      "Epoch 52/100\n",
      "1713/1713 [==============================] - 167s 97ms/step - loss: 2.1066\n",
      "Epoch 53/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 2.0656\n",
      "Epoch 54/100\n",
      "1713/1713 [==============================] - 167s 98ms/step - loss: 2.0461\n",
      "Epoch 55/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 2.0116\n",
      "Epoch 56/100\n",
      "1713/1713 [==============================] - 167s 98ms/step - loss: 1.9786\n",
      "Epoch 57/100\n",
      "1713/1713 [==============================] - 167s 98ms/step - loss: 1.9642\n",
      "Epoch 58/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.9245\n",
      "Epoch 59/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 1.8959\n",
      "Epoch 60/100\n",
      "1713/1713 [==============================] - 170s 99ms/step - loss: 1.8799\n",
      "Epoch 61/100\n",
      "1713/1713 [==============================] - 169s 99ms/step - loss: 1.8549\n",
      "Epoch 62/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 1.8232\n",
      "Epoch 63/100\n",
      "1713/1713 [==============================] - 170s 99ms/step - loss: 1.8034\n",
      "Epoch 64/100\n",
      "1713/1713 [==============================] - 167s 98ms/step - loss: 1.7750\n",
      "Epoch 65/100\n",
      "1713/1713 [==============================] - 167s 97ms/step - loss: 1.7587\n",
      "Epoch 66/100\n",
      "1713/1713 [==============================] - 170s 99ms/step - loss: 1.7370\n",
      "Epoch 67/100\n",
      "1713/1713 [==============================] - 167s 97ms/step - loss: 1.7192\n",
      "Epoch 68/100\n",
      "1713/1713 [==============================] - 167s 97ms/step - loss: 1.6922\n",
      "Epoch 69/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 1.6747\n",
      "Epoch 70/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 1.6652\n",
      "Epoch 71/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.6410\n",
      "Epoch 72/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.6249\n",
      "Epoch 73/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.6103\n",
      "Epoch 74/100\n",
      "1713/1713 [==============================] - 169s 99ms/step - loss: 1.5918\n",
      "Epoch 75/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.5667\n",
      "Epoch 76/100\n",
      "1713/1713 [==============================] - 167s 98ms/step - loss: 1.5529\n",
      "Epoch 77/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.5459\n",
      "Epoch 78/100\n",
      "1713/1713 [==============================] - 171s 100ms/step - loss: 1.5309\n",
      "Epoch 79/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.5145\n",
      "Epoch 80/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.4984\n",
      "Epoch 81/100\n",
      "1713/1713 [==============================] - 170s 99ms/step - loss: 1.4820\n",
      "Epoch 82/100\n",
      "1713/1713 [==============================] - 170s 99ms/step - loss: 1.4750\n",
      "Epoch 83/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.4659\n",
      "Epoch 84/100\n",
      "1713/1713 [==============================] - 170s 99ms/step - loss: 1.4512\n",
      "Epoch 85/100\n",
      "1713/1713 [==============================] - 174s 101ms/step - loss: 1.4312\n",
      "Epoch 86/100\n",
      "1713/1713 [==============================] - 166s 97ms/step - loss: 1.4275\n",
      "Epoch 87/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.4059\n",
      "Epoch 88/100\n",
      "1713/1713 [==============================] - 168s 98ms/step - loss: 1.3963\n",
      "Epoch 89/100\n",
      "1713/1713 [==============================] - 206s 120ms/step - loss: 1.3927\n",
      "Epoch 90/100\n",
      "1713/1713 [==============================] - 198s 116ms/step - loss: 1.3894\n",
      "Epoch 91/100\n",
      "1713/1713 [==============================] - 194s 113ms/step - loss: 1.3708\n",
      "Epoch 92/100\n",
      "1713/1713 [==============================] - 189s 110ms/step - loss: 1.3673\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1713/1713 [==============================] - 203s 119ms/step - loss: 1.3567\n",
      "Epoch 94/100\n",
      "1713/1713 [==============================] - 205s 120ms/step - loss: 1.3361\n",
      "Epoch 95/100\n",
      "1713/1713 [==============================] - 214s 125ms/step - loss: 1.3340\n",
      "Epoch 96/100\n",
      "1713/1713 [==============================] - 211s 123ms/step - loss: 1.3302\n",
      "Epoch 97/100\n",
      "1713/1713 [==============================] - 238s 139ms/step - loss: 1.3222\n",
      "Epoch 98/100\n",
      "1713/1713 [==============================] - 205s 119ms/step - loss: 1.3054\n",
      "Epoch 99/100\n",
      "1713/1713 [==============================] - 228s 133ms/step - loss: 1.3054\n",
      "Epoch 100/100\n",
      "1713/1713 [==============================] - 281s 164ms/step - loss: 1.2939\n"
     ]
    }
   ],
   "source": [
    "X, Y, max_len, total_words = dataset_preparation(text)\n",
    "model = create_model(X, Y, max_len, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/variable_length_lstm_2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-18d56e11e25d>:6: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"in love with mary jane she's my main thing can't hardly have by the top\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"in love with mary jane\", 10, max_len, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'light the doobie til it glow like a ruby on a train for'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"light the doobie\", 10, max_len, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
